---
title: 'Coursera Course: Pratical Machine Learning - Course Project'
author: "Genaro Costa"
date: "February, 19 2015"
output:
  html_document:
    keep_md: yes
---

The act of logging the activity some one perform on gim is a boring task. To tackle with that problem this report will use aceleromenter data, colected and provided by [1], to predict the user activity. The autor in [1] collected information from acelerometers placed on arm, forearm, belt and dumbbel among different users performing five different activities.. In this work we experiment such information to traing a prediction model and to try to predict which activity an user is performing based on its accelerometer data.

The code bellow setup the requirements libraries and configure the system for parallel performance.
```{r}
    library(lattice)
    library(ggplot2)
    library(caret)
    library(randomForest)
    library(RANN)
    library(plyr)
    library(ipred)
    library(doMC)
    
    
    ## set the locale to English to better deal with conversions.
    lastLocale <- Sys.setlocale(category = "LC_ALL", locale = "en_US.UTF-8")
    
    ## confire for multicore use
    registerDoMC(cores = 8)

    ## set the seed for reproducibility
    set.seed(13353)
```

##Data preprocessing

```{r}
    ## loads the data
    data <- read.csv(file = 'pml-training.csv', na.strings=c("","#DIV/0!","NA"))

    ## show some data dimensions
    dim(data)

    ## remove time and user related columns 
    cleanData <- data[,-which(names(data) %in% c("X","user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp"))]
    
    ## convert 'integer' and 'logic' types to 'numeric'
    for(i in seq(ncol(cleanData))) { 
            if(class(cleanData[1,i])=='integer' || class(cleanData[1,i])=='logical') {
                    cleanData[,i] <- as.numeric(cleanData[,i])
            }
    }
    ## remove near zero variability columns
    nsv <- nearZeroVar(cleanData)
    varData <- cleanData[,-nsv]
```

We loaded the data, considering the values ‘NA’ and ‘#DIV/0!’ as *NA* (missing values). The data has 160 columns. The columns related to time and user names where removed from the data. And all columns types different from *numeric* where converted to *numeric*. We also removed the near zero variability variables from the dataset. That reduces the variables to 119.

##Machine learning model generation

The machine learning algotithms used are provided by the *caret* R package. The data were divided in two different sets for cross-validation. The training and testing sets has 70% and 30% respectively. 

```{r}
    inTrain <- createDataPartition(varData$classe, p=.7, list = FALSE)
    training <- varData[inTrain,]
    testing <- varData[-inTrain,]
```

We first use the Random Forest algorithm in the training phase. As we have a lot of missing values, we use the knnInpute algorithm to fill the values missing.

```{r}
    ## if there is a cached model, loads it
    if(file.exists('models.bin')) { 
        load('models.bin')
    }
    if(!exists('modFit')) {
        ## train only if the models doesnot exists.
        modFit <- train(classe ~ ., data=training, method="rf", preProcess = c("knnImpute"), na.action  = na.pass)
    }

    ## performs crossvalidation
    prediction <- predict(modFit, testing, na.action  = na.pass)

    ## get the confusion matrix
    confusionMatrix(prediction, testing$classe)
    
    ## calculate the accuracy.
    accuracy <- sum(prediction==testing$classe)/nrow(testing)
```

As presented in the confusion matrix above, we have a good accuracy (`r round(accuracy*100, digits=2)`%). To find out if we could get a better accuracy, we used bagging on random forest algorithm. In that scenarion we have an **out of sample error** of `r round((1-accuracy)*100, digits=2)`%.

```{r}
    if(!exists('modFit2')) {
        ## train only if the models doesnot exists.
        modFit2 <- train(classe ~ ., data=training, method="treebag", preProcess = c("knnImpute"), na.action  = na.pass)
    }
    
    ## performs crossvalidation
    prediction2 <- predict(modFit2, testing, na.action  = na.pass)
    
    ## get the confusion matrix
    confusionMatrix(prediction2, testing$classe)

    ## calculate the accuracy.
    accuracy2 <- sum(prediction2==testing$classe)/nrow(testing)
```

As presented by the confusion matrix above, when using bagging technique, we got a little better accuracy (`r round(accuracy2*100, digits=2)`%) than the accuracy (`r round(accuracy*100, digits=2)`%) achieved when using only the random forest algorithm. Using bagging we have an **out of sample error** of `r round((1-accuracy2)*100, digits=2)`%. 

##References
[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.